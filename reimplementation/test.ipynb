{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_biased_dataset(train_ratio=0.95,test_ratio=0.5,Ntrain=1000,Nvalid=10,Ntest=100,seed=None,class0=4,class1=9):\n",
    "    \n",
    "    # fix the random seed for the fair comparison with baseline\n",
    "    generator   =   np.random.seed(seed)\n",
    "    \n",
    "    # use imbalanced training data set / balanced test data set\n",
    "    (train_images,train_labels),(test_images,test_labels)   =   tf.keras.datasets.mnist.load_data()\n",
    "    train_images_class0     =   train_images[train_labels==class0]\n",
    "    test_images_class0      =   test_images[test_labels==class0]\n",
    "    train_images_class1     =   train_images[train_labels==class1]\n",
    "    test_images_class1      =   test_images[test_labels==class1]\n",
    "    \n",
    "    # dtype conversion\n",
    "    train_images_class0     =   train_images_class0.astype(np.float32)/255\n",
    "    test_images_class0      =   test_images_class0.astype(np.float32)/255\n",
    "    train_images_class1     =   train_images_class1.astype(np.float32)/255\n",
    "    test_images_class1      =   test_images_class1.astype(np.float32)/255\n",
    "    \n",
    "    # random selection\n",
    "    print('Randomly select %d train / %d validation / %d test data in class %d' % (int(np.floor(train_ratio*Ntrain)),int(np.floor(test_ratio*Nvalid)),int(np.floor(test_ratio*Ntest)),class0))\n",
    "    train_images_class0_sel     =   train_images_class0[np.random.randint(0,train_images_class0.shape[0],int(np.floor(train_ratio*Ntrain))),:,:]\n",
    "    valid_images_class0_sel     =   train_images_class0[np.random.randint(0,train_images_class0.shape[0],int(np.floor(test_ratio*Nvalid))),:,:]\n",
    "    test_images_class0_sel      =   test_images_class0[np.random.randint(0,test_images_class0.shape[0],int(np.floor(test_ratio*Ntest))),:,:]\n",
    "\n",
    "    print('Randomly select %d train / %d validation / %d test data in class %d' % (int(Ntrain-np.floor(train_ratio*Ntrain)),int(Nvalid-np.floor(test_ratio*Nvalid)),Ntest-int(np.floor(test_ratio*Ntest)),class1))\n",
    "    train_images_class1_sel     =   train_images_class1[np.random.randint(0,train_images_class1.shape[0],int(Ntrain-np.floor(train_ratio*Ntrain))),:,:]\n",
    "    valid_images_class1_sel     =   train_images_class1[np.random.randint(0,train_images_class1.shape[0],int(Nvalid-np.floor(test_ratio*Nvalid))),:,:]\n",
    "    test_images_class1_sel      =   test_images_class1[np.random.randint(0,test_images_class1.shape[0],int(Ntest-np.floor(test_ratio*Ntest))),:,:]\n",
    "    \n",
    "    # concatenate class 0 and class 1 data\n",
    "    x_train_concatenate         =   np.concatenate([train_images_class0_sel,train_images_class1_sel],axis=0)\n",
    "    x_valid_concatenate         =   np.concatenate([valid_images_class0_sel,valid_images_class1_sel],axis=0)\n",
    "    x_test_concatenate          =   np.concatenate([test_images_class0_sel,test_images_class1_sel],axis=0)\n",
    "\n",
    "    y_train_concatenate         =   np.concatenate([np.zeros([train_images_class0_sel.shape[0]]),np.ones([train_images_class1_sel.shape[0]])])\n",
    "    y_valid_concatenate         =   np.concatenate([np.zeros([valid_images_class0_sel.shape[0]]),np.ones([valid_images_class1_sel.shape[0]])])\n",
    "    y_test_concatenate          =   np.concatenate([np.zeros([test_images_class0_sel.shape[0]]),np.ones([test_images_class1_sel.shape[0]])])\n",
    "\n",
    "    # reshape for CONV2D\n",
    "    x_train_concatenate         =   x_train_concatenate.reshape(-1,x_train_concatenate.shape[1],x_train_concatenate.shape[2],1)\n",
    "    x_valid_concatenate         =   x_valid_concatenate.reshape(-1,x_valid_concatenate.shape[1],x_valid_concatenate.shape[2],1)\n",
    "    x_test_concatenate          =   x_test_concatenate.reshape(-1,x_test_concatenate.shape[1],x_test_concatenate.shape[2],1)\n",
    "\n",
    "    # shuffle for training\n",
    "    idx     =   np.arange(x_train_concatenate.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train         =   x_train_concatenate[idx]\n",
    "    y_train         =   y_train_concatenate[idx]\n",
    "\n",
    "    idx     =   np.arange(x_valid_concatenate.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_valid         =   x_valid_concatenate[idx]\n",
    "    y_valid         =   y_valid_concatenate[idx]\n",
    "\n",
    "    idx     =   np.arange(x_test_concatenate.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_test          =   x_test_concatenate[idx]\n",
    "    y_test          =   y_test_concatenate[idx]\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_model(inputs,labels,theta_dict=None,w=None,reuse=None,dtype=tf.float32):\n",
    "    \n",
    "    # check whether the parameters of the nueral network is specified, if not specified initialize with empty dictionary\n",
    "    if theta_dict is None:\n",
    "        theta_dict  =   {}\n",
    "\n",
    "    def _get_var(name,shape,dtype,initializer):\n",
    "        key     =   tf.get_variable_scope().name + '/' + name\n",
    "        \n",
    "        # check whether the given variable is created\n",
    "        if key in theta_dict:\n",
    "            return theta_dict[key]\n",
    "        else:\n",
    "            # create a new variable\n",
    "            var             =   tf.get_variable(name,shape,dtype,initializer=initializer)\n",
    "            theta_dict[key] =   var\n",
    "            return var\n",
    "\n",
    "    with tf.variable_scope('Model',reuse=reuse):\n",
    "        inputs_     =   tf.cast(tf.reshape(inputs,[-1,28,28,1]),dtype)\n",
    "        labels      =   tf.cast(labels,dtype)\n",
    "\n",
    "        # create weight\n",
    "        theta_init  =   tf.truncated_normal_initializer(mean=0,stddev=0.1)\n",
    "        theta1      =   _get_var('theta1',[5,5,1,16],dtype,initializer=theta_init)\n",
    "        theta2      =   _get_var('theta2',[5,5,16,32],dtype,initializer=theta_init)\n",
    "        theta3      =   _get_var('theta3',[5,5,32,64],dtype,initializer=theta_init)\n",
    "        theta4      =   _get_var('theta4',[1024,100],dtype,initializer=theta_init)\n",
    "        theta5      =   _get_var('theta5',[100,1], dtype,initializer=theta_init)\n",
    "\n",
    "        # create offset\n",
    "        b_init      =   tf.constant_initializer(0.0)\n",
    "        b1          =   _get_var('b1',[16],dtype,initializer=b_init)\n",
    "        b2          =   _get_var('b2',[32],dtype,initializer=b_init)\n",
    "        b3          =   _get_var('b3',[64],dtype,initializer=b_init)\n",
    "        b4          =   _get_var('b4',[100],dtype,initializer=b_init)\n",
    "        b5          =   _get_var('b5',[1],dtype,initializer=b_init)\n",
    "        \n",
    "        # create layers\n",
    "        # convolution layer level 1\n",
    "        #l0      =   tf.identity(inputs_,name='l0')\n",
    "        z1  =   tf.add(tf.nn.conv2d(inputs_,theta1,[1,1,1,1],'SAME'),b1,name='z1')\n",
    "        l1  =   tf.nn.relu(tf.nn.max_pool(z1,[1,3,3,1],[1,2,2,1],'SAME'),name='l1')\n",
    "\n",
    "        # convolution layer level 2\n",
    "        z2  =   tf.add(tf.nn.conv2d(l1,theta2,[1,1,1,1],'SAME'),b2,name='i2')\n",
    "        l2  =   tf.nn.relu(tf.nn.max_pool(z2,[1,3,3,1],[1,2,2,1],'SAME'),name='l2')\n",
    "\n",
    "        # convolution layer level 3\n",
    "        z3  =   tf.add(tf.nn.conv2d(l2,theta3,[1,1,1,1],'SAME'),b3,name='z3')\n",
    "        l3  =   tf.nn.relu(tf.nn.max_pool(z3,[1,3,3,1],[1,2,2,1],'SAME'),name='l3')\n",
    "\n",
    "        # fully connected layer level 4\n",
    "        z4  =   tf.add(tf.matmul(tf.reshape(l3,[-1,1024]),theta4),b4,name='z4')\n",
    "        l4  =   tf.nn.relu(z4,name='l4')\n",
    "\n",
    "        # FC-5\n",
    "        z5  =   tf.add(tf.matmul(l4,theta5),b5,name='z5')\n",
    "\n",
    "        logits  =   tf.squeeze(z5)\n",
    "        out     =   tf.sigmoid(logits)\n",
    "        \n",
    "        # multiply sample weight\n",
    "        if w is None:\n",
    "            # average loss\n",
    "            loss    =   tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        else:\n",
    "            # weighted loss\n",
    "            loss    =   tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)*w)\n",
    "    return theta_dict, loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_autodiff(input_bias,label_bias,input_valid,label_valid,bsize_bias,bsize_valid,eps=0.0,gate_gradients=1):\n",
    "\n",
    "    w_bias      =   tf.zeros([bsize_bias],dtype=tf.float32)\n",
    "    w_valid     =   tf.ones([bsize_valid],dtype=tf.float32)/float(bsize_valid)\n",
    "    \n",
    "    [theta_dict,loss_bias,logits_bias] = LeNet_model(input_bias,label_bias,w=w_bias,reuse=True)\n",
    "    var_names   =   theta_dict.keys()\n",
    "    var_list    =   [theta_dict[idx] for idx in var_names]\n",
    "    grads       =   tf.gradients(loss_bias,var_list,gate_gradients=gate_gradients)\n",
    "\n",
    "    var_list_new    =   [vv-gg for gg,vv in zip(grads,var_list)]\n",
    "    theta_dict_new  =   dict(zip(var_names,var_list_new))\n",
    "    [_,loss_valid,logits_valid] = LeNet_model(input_valid,label_valid,theta_dict=theta_dict_new,w=w_valid,reuse=True)\n",
    "    grads_w         =   tf.gradients(loss_valid,[w_bias],gate_gradients=gate_gradients)[0]\n",
    "    ex_weight       =   -grads_w\n",
    "    ex_weight_plus  =   tf.maximum(ex_weight,eps)\n",
    "    ex_weight_sum   =   tf.reduce_sum(ex_weight_plus)\n",
    "    ex_weight_sum   +=  tf.to_float(tf.equal(ex_weight_sum, 0.0))\n",
    "    ex_weight_norm  =   ex_weight_plus/ex_weight_sum\n",
    "    \n",
    "    return ex_weight_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(x,y,Nepoch=0,cur_idx=0,shuffle=False,bsize=100):\n",
    "    \n",
    "    if (Nepoch == 0) and (cur_idx==0) and shuffle:\n",
    "        idx     =   np.arange(x.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        x       =   x[idx,:,:]\n",
    "        y       =   y[idx]\n",
    "        \n",
    "    if cur_idx+bsize <= x.shape[0]:\n",
    "        xbatch  =   x[cur_idx:cur_idx+bsize,:,:]\n",
    "        ybatch  =   y[cur_idx:cur_idx+bsize]\n",
    "        cur_idx =   cur_idx+bsize\n",
    "        \n",
    "    else:\n",
    "        get_samples     =   x.shape[0]-cur_idx\n",
    "        xbatch_temp     =   x[cur_idx:x.shape[0],:,:]\n",
    "        ybatch_temp     =   y[cur_idx:x.shape[0]]\n",
    "        \n",
    "        if shuffle:\n",
    "            idx     =   np.arange(x.shape[0])\n",
    "            np.random.shuffle(idx)\n",
    "            x       =   x[idx,:,:]\n",
    "            y       =   y[idx]\n",
    "            \n",
    "        Nepoch          +=  1\n",
    "        left_samples    =   bsize-get_samples\n",
    "        xbatch          =   np.concatenate((xbatch_temp,x[0:left_samples,:,:]),axis=0)\n",
    "        ybatch          =   np.concatenate((ybatch_temp,y[0:left_samples]),axis=0)\n",
    "        cur_idx         =   left_samples\n",
    "        \n",
    "    return xbatch, ybatch, x, y, Nepoch, cur_idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly select 990 train / 5 validation / 50 test data in class 4\n",
      "Randomly select 10 train / 5 validation / 50 test data in class 9\n",
      "WARNING:tensorflow:From c:\\Users\\mhcho\\miniforge3\\envs\\stat598\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "Iter:  5 Loss:  1.8708231 Test accuracy:  5.0\n",
      "Iter:  10 Loss:  1.5892813 Test accuracy:  0.5\n",
      "Iter:  15 Loss:  0.90064424 Test accuracy:  0.75\n",
      "Iter:  20 Loss:  0.7877479 Test accuracy:  0.8899999856948853\n",
      "Iter:  25 Loss:  0.7388252 Test accuracy:  0.8600000143051147\n",
      "Iter:  30 Loss:  0.64036626 Test accuracy:  0.9599999785423279\n",
      "Iter:  35 Loss:  0.63818073 Test accuracy:  0.9300000071525574\n",
      "Iter:  40 Loss:  0.57269514 Test accuracy:  0.9599999785423279\n",
      "Iter:  45 Loss:  0.5824463 Test accuracy:  0.949999988079071\n",
      "Iter:  50 Loss:  0.5439563 Test accuracy:  0.9900000095367432\n",
      "Final Loss:  0.5439563 Test accuracy:  0.9900000095367432\n"
     ]
    }
   ],
   "source": [
    "# create imbalanced datatset\n",
    "# set the size of training/validation/test sets\n",
    "Ntrain  =   1000\n",
    "Nvalid  =   10\n",
    "Ntest   =   100\n",
    "\n",
    "# adjust the ratio of trainig/validation/test samples\n",
    "train_ratio =   0.99\n",
    "test_ratio  =   0.50\n",
    "\n",
    "# get the trainig/validation/test samples\n",
    "[x_train,y_train,x_valid,y_valid,x_test,y_test] =   get_random_biased_dataset(train_ratio,test_ratio,Ntrain,Nvalid,Ntest,seed=None,class0=4,class1=9)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # set learning rate, batch size,\n",
    "    lr      =   1e-3\n",
    "    moment  =   0.9\n",
    "    bsize   =   100\n",
    "    \n",
    "    # place holder for DNN model input/output\n",
    "    x_      =   tf.placeholder(tf.float32,[None,x_train.shape[1],x_train.shape[2],1],name='x')\n",
    "    y_      =   tf.placeholder(tf.float32,[None],name='y')\n",
    "    x_val_  =   tf.placeholder(tf.float32,[None,x_valid.shape[1],x_valid.shape[2],1],name='x_val')\n",
    "    y_val_  =   tf.placeholder(tf.float32,[None],name='y_val')\n",
    "    w_      =   tf.placeholder(tf.float32,[None],name='w')\n",
    "    lr_     =   tf.placeholder(tf.float32,[],name='lr')\n",
    "\n",
    "    # build training model\n",
    "    with tf.name_scope('Train'):\n",
    "        [_,loss_c,logits_c]     =   LeNet_model(x_,y_,theta_dict=None,w=w_,reuse=None,dtype=tf.float32)\n",
    "        train_op                =   tf.train.MomentumOptimizer(lr,moment).minimize(loss_c)\n",
    "\n",
    "    # build evaluation model\n",
    "    with tf.name_scope('Val'):\n",
    "        [_,loss_eval,logits_eval]   =   LeNet_model(x_,y_,theta_dict=None,w=w_,reuse=True,dtype=tf.float32)\n",
    "        prediction_                 =   tf.cast(tf.sigmoid(logits_eval)>0.5,tf.float32)\n",
    "        acc_                        =   tf.reduce_mean(tf.cast(tf.equal(prediction_,y_),tf.float32))\n",
    "\n",
    "    # build reweighting model\n",
    "    w_values_   =   reweight_autodiff(x_, y_,x_val_,y_val_,bsize,min(bsize,Nvalid),eps=0.0,gate_gradients=1)\n",
    "    \n",
    "    # training process parameters\n",
    "    Nepoch_train    =   0\n",
    "    train_cur_idx   =   0\n",
    "    Nepoch_valid    =   0\n",
    "    valid_cur_idx   =   0\n",
    "    \n",
    "    # measure metric\n",
    "    acc_train_sum   =   0.0\n",
    "    acc_test_sum    =   0.0\n",
    "    \n",
    "    # train\n",
    "    iter_target     =   50\n",
    "    iter            =   0\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for iter in range(iter_target):\n",
    "        [x_minibatch,y_minibatch,x_train,y_train,Nepoch_train,train_cur_idx]            =   get_next_batch(x_train,y_train,Nepoch_train,train_cur_idx,True,bsize)\n",
    "        [x_val_minibatch,y_val_minibatch,x_valid,y_valid,Nepoch_valid,valid_cur_idx]    =   get_next_batch(x_valid,y_valid,Nepoch_valid,valid_cur_idx,True,min(bsize,Nvalid))\n",
    "\n",
    "        w_value         =   sess.run(w_values_,feed_dict={x_:x_minibatch,y_:y_minibatch,x_val_:x_val_minibatch,y_val_:y_val_minibatch})\n",
    "        [loss,acc,_]    =   sess.run([loss_c,acc_,train_op],feed_dict={x_:x_minibatch,y_:y_minibatch,x_val_:x_val_minibatch,y_val_:y_val_minibatch,w_:w_value,lr_:lr}) # np.ones((bsize),dtype=np.float32)*np.float32(1/bsize)\n",
    "\n",
    "        if (iter+1) % 5 == 0:\n",
    "            acc_train_sum   =   0.0\n",
    "            acc_test_sum    =   0.0\n",
    "            \n",
    "            Nepoch_test     =   0\n",
    "            test_cur_idx    =   0\n",
    "            \n",
    "            for step in range(x_train.shape[0]//bsize):\n",
    "                [x_test_minibatch,y_test_minibatch,x_train,y_train,Nepoch_test,test_cur_idx]    =   get_next_batch(x_test,y_test,Nepoch_test,test_cur_idx,True,bsize)\n",
    "                acc             =   sess.run(acc_,feed_dict={x_:x_test_minibatch, y_:y_test_minibatch})\n",
    "                acc_test_sum    +=  acc\n",
    "\n",
    "            test_acc    =   acc_test_sum/float(x_train.shape[0]//bsize)\n",
    "            print('Iter: ',iter+1,'Loss: ', loss,'Test accuracy: ',test_acc)\n",
    "            \n",
    "    acc_train_sum   =   0.0\n",
    "    acc_test_sum    =   0.0\n",
    "    \n",
    "    Nepoch_test     =   0\n",
    "    test_cur_idx    =   0\n",
    "    \n",
    "    for step in range(x_train.shape[0]//bsize):\n",
    "        [x_test_minibatch,y_test_minibatch,x_train,y_train,Nepoch_test,test_cur_idx]    =   get_next_batch(x_test,y_test,Nepoch_test,test_cur_idx,True,bsize)\n",
    "        acc             =   sess.run(acc_,feed_dict={x_:x_test_minibatch, y_: y_test_minibatch})\n",
    "        acc_test_sum    +=  acc\n",
    "\n",
    "    test_acc    =   acc_test_sum/float(x_train.shape[0]//bsize)\n",
    "    print('Final','Loss: ', loss,'Test accuracy: ',test_acc) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat598",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
